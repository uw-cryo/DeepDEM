{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to use train a convolutional neural network for DEM refinement. We leverage `pytorch`, `pytorch-lightning`, `torchgeo` as part of our ML stack. Our model follows the `UNet` architecture and incorporates a few variations in the form of `ResNet` backbones and additional skip connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# misc imports\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# pytorch imports\n",
    "from torch import nn \n",
    "import torch\n",
    "\n",
    "# pytorch-lightning imports\n",
    "import lightning as L\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "# torchgeo imports\n",
    "from torchgeo.datasets import stack_samples\n",
    "\n",
    "# Kornia imports for data augmentation\n",
    "import kornia.augmentation as K\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, str(Path('.').absolute().parent))\n",
    "\n",
    "# local imports\n",
    "from scripts.task_module import DeepDEMRegressionTask\n",
    "from scripts.dataset_modules import CustomInputDataset, CustomDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set file path to pre-processed data\n",
    "data_path = Path('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training parameters\n",
    "CHIP_SIZE = 256 \n",
    "BATCH_SIZE = 12\n",
    "NUM_WORKERS = 8\n",
    "LR = 5e-4\n",
    "\n",
    "bands = [\n",
    "    \"asp_dsm\",\n",
    "    \"ortho_left\",\n",
    "    \"ortho_right\",\n",
    "    \"ndvi\",\n",
    "    \"nodata_mask\",\n",
    "    \"triangulation_error\",\n",
    "    \"lidar_data\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to data\n",
    "datapath = '/mnt/1.0_TB_VOLUME/karthikv/DeepDEM/data/baker_csm_stack/processed_rasters/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = nn.Sequential(\n",
    "    K.RandomHorizontalFlip(p=0.5),\n",
    "    K.RandomVerticalFlip(p=0.5),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule_params = {\n",
    "    'paths': datapath,\n",
    "    'dataset_class':CustomDataModule,\n",
    "    'chip_size':CHIP_SIZE,\n",
    "    'batch_size':BATCH_SIZE,\n",
    "    'num_workers':NUM_WORKERS,\n",
    "    'collate_fn':stack_samples,\n",
    "    'cuda': torch.cuda.is_available(),\n",
    "    'bands':bands,\n",
    "    'train_aug':transforms\n",
    "}\n",
    "datamodule = CustomDataModule(**datamodule_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdataset = CustomInputDataset(paths=datapath, bands=bands)\n",
    "left_ortho_mean, left_ortho_std = tempdataset.compute_mean_std(\"ortho_left\")\n",
    "right_ortho_mean, right_ortho_std = tempdataset.compute_mean_std(\"ortho_right\")\n",
    "\n",
    "model_kwargs = {\n",
    "    'model':'smp-unet',\n",
    "    'encoder':'resnet18',\n",
    "    'encoder_weights':'imagenet',\n",
    "    'bands':bands,\n",
    "    'left_ortho_mean':left_ortho_mean,\n",
    "    'left_ortho_std':left_ortho_std,\n",
    "    'right_ortho_mean':right_ortho_mean,\n",
    "    'right_ortho_std':right_ortho_std,\n",
    "    'chip_size':CHIP_SIZE,\n",
    "    'do_BN':False,\n",
    "    'bias_conv_layer':False,\n",
    "    'lr':LR,\n",
    "    'patience':10,\n",
    "    'num_workers':NUM_WORKERS,\n",
    "    'max_epochs':100,\n",
    "    'lr_scheduler':True,\n",
    "    'lr_scheduler_scale_factor':0.5,\n",
    "    'lr_scheduler_patience':150\n",
    "}\n",
    "task = DeepDEMRegressionTask(**model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup folder for logging\n",
    "checkpoint_directory = Path(f'./checkpoints/checkpoint_directory_{datetime.now().strftime(\"%Y%m%d\")}')\n",
    "checkpoint_directory.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "model_count = len([x for x in list(checkpoint_directory.glob('*')) if x.is_dir()]) + 1 # incase multiple runs exist in the folder\n",
    "\n",
    "checkpoint_directory = checkpoint_directory / f\"version_{str(model_count).zfill(3)}\"\n",
    "checkpoint_directory.mkdir(exist_ok=False)\n",
    "\n",
    "callbacks =[LearningRateMonitor(logging_interval='step'), ModelCheckpoint(dirpath=checkpoint_directory, monitor='val_loss', mode='min')]\n",
    "logger = TensorBoardLogger(save_dir=\"logs/\", name=f\"my_experiment_{datetime.now().strftime(\"%Y%m%d\")}\")\n",
    "\n",
    "trainer = L.Trainer(accelerator = \"gpu\" if torch.cuda.is_available() else \"cpu\", \n",
    "                    default_root_dir=checkpoint_directory, \n",
    "                    max_epochs=model_kwargs['max_epochs'], logger=logger, check_val_every_n_epoch=1, # type: ignore\n",
    "                    log_every_n_steps=1, fast_dev_run=False, # set fast_dev_run to True to do quick sanity check run (no training)\n",
    "                    callbacks=callbacks) # type: ignore\n",
    "\n",
    "# Train model\n",
    "trainer.fit(model=task, datamodule=datamodule)\n",
    "\n",
    "# Save model weights\n",
    "torch.save(task.model.state_dict(), checkpoint_directory/f\"model_weights_version{model_count}.pth\")\n",
    "\n",
    "print(f\"Model weights saved to {checkpoint_directory/f\"model_weights_version{model_count}.pth\"}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training progress can be monitored by running tensorboard using the following commands:\n",
    "\n",
    "`\n",
    "tensorboard --logdir='./tensorboard_dirs' --port=<specify port number of choice>\n",
    "`\n",
    "\n",
    "and then navigating to the following URL in a browser:\n",
    "\n",
    "`\n",
    "http://localhost:<port number>/\n",
    "`\n",
    "\n",
    "If the training is happening on a remote machine, we can open an ssh connection to the remote machine:\n",
    "\n",
    "`\n",
    "ssh -N -f -L <local port>:127.0.0.1:<remote machine port> <username>@<server>\n",
    "`\n",
    "\n",
    "Followed by accessing `http://localhost:<local port>/`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_dem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
