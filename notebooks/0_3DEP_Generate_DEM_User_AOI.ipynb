{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dda9bb9",
   "metadata": {},
   "source": [
    "# Generate DSM from USGS 3D Elevation Program (3DEP) lidar data corresponding to Worldview Mount Baker imagery to train DeepDEM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f1a034",
   "metadata": {},
   "source": [
    "This notebook has been repurposed from the OpenTopography GitHub repository [here](https://github.com/OpenTopography/OT_3DEP_Workflows/blob/main/notebooks/01_3DEP_Generate_DEM_User_AOI.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9692c43b",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "This notebook enables users to download USGS LIDAR data from Amazon Web Services (AWS) S3 public bucket and process them for use with DeepDEM as training data.\n",
    "\n",
    "#### Specific features of this notebook\n",
    "\n",
    "1. The notebook reads in the bounds of the Worldview imagery over which we would like to generate a DSM\n",
    "\n",
    "2. Send an API request to <a href=\"https://registry.opendata.aws/usgs-lidar/\"> Amazon Web Services (AWS) EPT (Entwine Point Tile) S3 bucket</a> returns 3DEP point cloud data within user-defined AOI. \n",
    "\n",
    "3. Process returned pointcloud using PDAL.\n",
    "\n",
    "4. Create a Digital Surface Model (DSM) with user-specifed resolution, gridding method, and file type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7513dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Import Modules\n",
    "# Misc imports \n",
    "import copy\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# GIS imports\n",
    "import geopandas as gpd\n",
    "from osgeo import gdal\n",
    "import pyproj\n",
    "from shapely.geometry import shape, Point, Polygon\n",
    "from shapely.ops import transform\n",
    "# Plotting imports\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import ipyleaflet\n",
    "\n",
    "# PDAL imports\n",
    "import pdal\n",
    "\n",
    "from pathlib import Path\n",
    "import rasterio\n",
    "\n",
    "from osgeo import gdal\n",
    "import ipywidgets as widgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d924c977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj_to_3857(poly, orig_crs):\n",
    "    \"\"\"\n",
    "    Function for reprojecting a polygon from a shapefile of any CRS to Web Mercator (EPSG: 3857).\n",
    "    The original polygon must have a CRS assigned.\n",
    "    \n",
    "    Parameters:\n",
    "        poly (shapely polygon): User area of interest (AOI)\n",
    "        orig_crs (str): the original CRS (EPSG) for the shapefile. It is stripped out during import_shapefile_to_shapely() method\n",
    "\n",
    "    Returns:\n",
    "        user_poly_proj4326 (shapely polygon): User AOI in EPSG 4326\n",
    "        user_poly_proj3857 (shapely polygon): User AOI in EPSG 3857\n",
    "    \"\"\"\n",
    "    wgs84 = pyproj.CRS(\"EPSG:4326\")\n",
    "    web_mercator = pyproj.CRS(\"EPSG:3857\")\n",
    "    project_gcs = pyproj.Transformer.from_crs(orig_crs, wgs84, always_xy=True).transform\n",
    "    project_wm = pyproj.Transformer.from_crs(orig_crs, web_mercator, always_xy=True).transform\n",
    "    user_poly_proj4326 = transform(project_gcs, poly)\n",
    "    user_poly_proj3857 = transform(project_wm, poly)\n",
    "    return(user_poly_proj4326, user_poly_proj3857)\n",
    "\n",
    "def gcs_to_proj(poly):\n",
    "    \"\"\"\n",
    "    Function for reprojecting polygon shapely object from geographic coordinates (EPSG:4326) \n",
    "    to Web Mercator (EPSG: 3857)). \n",
    "    \n",
    "    Parameters:\n",
    "        poly (shapely polygon): User area of interest (AOI)\n",
    "\n",
    "    Returns:\n",
    "        user_poly_proj3857 (shapely polygon): User AOI in EPSG 3857\n",
    "    \"\"\"\n",
    "    wgs84 = pyproj.CRS(\"EPSG:4326\")\n",
    "    web_mercator = pyproj.CRS(\"EPSG:3857\")\n",
    "    project = pyproj.Transformer.from_crs(wgs84, web_mercator, always_xy=True).transform\n",
    "    user_poly_proj3857 = transform(project, poly)\n",
    "    return(user_poly_proj3857)\n",
    "\n",
    "def import_shapefile_to_shapely(path):\n",
    "    \"\"\"\n",
    "    Conversion of shapefile to shapely object.\n",
    "    \n",
    "    Parameters:\n",
    "        path (filepath): location of shapefile on user's local file system\n",
    "\n",
    "    Returns:\n",
    "        user_AOI (shapely polygon): User AOI\n",
    "    \"\"\"\n",
    "    shapefile_path = path\n",
    "    gdf = gpd.read_file(shapefile_path)\n",
    "    orig_crs = gdf.crs                   # this is the original CRS of the imported shapefile\n",
    "    user_shp = gdf.loc[0, 'geometry']\n",
    "    user_shp_epsg4326, user_shp_epsg3857 = proj_to_3857(user_shp, orig_crs)\n",
    "    user_AOI = [[user_shp_epsg4326, user_shp_epsg3857]]\n",
    "    return user_AOI\n",
    "    \n",
    "def handle_draw(target, action, geo_json):\n",
    "    \"\"\"\n",
    "    Functionality to draw area of interest (AOI) on interactive ipyleaflet map.\n",
    "    \n",
    "    Parameters:\n",
    "        extent_epsg3857 (shapely polygon): Polygon of user-defined AOI\n",
    "        usgs_3dep_dataset_name (str): Name of 3DEP dataset which AOI overlaps\n",
    "        resolution (float): The desired resolution of the pointcloud based on the following definition:\n",
    "    \"\"\"\n",
    "        \n",
    "    geom = dict(geo_json['geometry'])\n",
    "    user_poly = shape(geom)\n",
    "    user_poly_proj3857 = gcs_to_proj(user_poly)\n",
    "    print('AOI is valid and has boundaries of ', user_poly_proj3857.bounds, 'Please proceed to the next cell.')\n",
    "    user_AOI.append((user_poly, user_poly_proj3857))  #for various reasons, we need user AOI in GCS and EPSG 3857\n",
    "    \n",
    "def downsample_dem(dem):\n",
    "    \"\"\"\n",
    "    Function for evaluating whether DEM should be downsampled prior to plotting. If dem.shape is larger than target.shape, the dem is downsampled.\n",
    "\n",
    "    Parameters:\n",
    "        dem (array): 2-D numpy array representing the dem data\n",
    "\n",
    "    Returns: \n",
    "        down_sampled (array): Downsampled 2-D numpy array (if dimensions exceed target dimensions)\n",
    "        OR\n",
    "        dem (array): Original 2-D numpy array (if downsampling is not needed)\n",
    "    \"\"\"\n",
    "    target_shape = tuple((1000,1000))   # if either dimension is larger than 1000 pixels, the dem will be downsampled\n",
    "    scale_factors = [dim_target / dim_input for dim_target, dim_input in zip(target_shape, dem.shape)] \n",
    "    \n",
    "    if any(factor < 1 for factor in scale_factors):\n",
    "        if scale_factors[0] < 1:\n",
    "            new_width = dem.rio.width * scale_factors[0]\n",
    "        else:\n",
    "            new_width = dem.rio.width\n",
    "        if scale_factors[1] < 1:\n",
    "            new_height = dem.rio.height * scale_factors[1]\n",
    "        else:\n",
    "            new_height = dem.rio.height\n",
    "\n",
    "        # Downsample DTM/DSM\n",
    "        down_sampled = dem.rio.reproject(dem.rio.crs, shape=(int(new_height), int(new_width)), resampling=Resampling.bilinear)\n",
    "        \n",
    "        return down_sampled\n",
    "    \n",
    "    else:\n",
    "        return dem\n",
    "    \n",
    "def build_pdal_pipeline(extent_epsg3857, usgs_3dep_dataset_names, pc_resolution, filterNoise = False,\n",
    "                        reclassify = False, savePointCloud = True, outCRS = 3857, pc_outName = 'filter_test', \n",
    "                        pc_outType = 'laz'):\n",
    "\n",
    "    \"\"\"\n",
    "    Build pdal pipeline for requesting, processing, and saving point cloud data. Each processing step is a 'stage' \n",
    "    in the final pdal pipeline. Each stages is appended to the 'pointcloud_pipeline' object to produce the final pipeline.\n",
    "    \n",
    "    Parameters:\n",
    "    extent_epsg3857 (shapely polygon): Polygon for user-defined AOI in Web Mercator projection (EPS:3857)Polygon is generated \n",
    "                            either through the 'handle_draw' methor or by inputing their own shapefile.\n",
    "    usgs_3dep_dataset_names (str): List of name of the 3DEP dataset(s) that the data will be obtained. This parameter is set \n",
    "                                determined through intersecttino of the 3DEP and AOI polys.\n",
    "    pc_resolution (float): The desired resolution of the pointcloud based on the following definition:\n",
    "        \n",
    "                        Source: https://pdal.io/stages/readers.ept.html#readers-ept\n",
    "                            A point resolution limit to select, expressed as a grid cell edge length. \n",
    "                            Units correspond to resource coordinate system units. For example, \n",
    "                            for a coordinate system expressed in meters, a resolution value of 0.1 \n",
    "                            will select points up to a ground resolution of 100 points per square meter.\n",
    "                            The resulting resolution may not be exactly this value: the minimum possible \n",
    "                            resolution that is at least as precise as the requested resolution will be selected. \n",
    "                            Therefore the result may be a bit more precise than requested.\n",
    "                            \n",
    "    filterNoise (bool): Option to remove points from USGS Class 7 (Low Noise) and Class 18 (High Noise).\n",
    "    reclassify (bool): Option to remove USGS classes and run SMRF to classify ground points only. Default == False.\n",
    "    savePointCloud (bool): Option to save (or not) the point cloud data. If savePointCloud == False, \n",
    "           the pc_outName and pc_outType parameters are not used and can be any value.\n",
    "    outCRS (int): Output coordinate reference systemt (CRS), specified by ESPG code (e.g., 3857 - Web Mercator)\n",
    "    pc_outName (str): Desired name of file on user's local file system. If savePointcloud = False, \n",
    "                  pc_outName can be in value.\n",
    "    pc_outType (str):  Desired file extension. Input must be either 'las' or 'laz'. If savePointcloud = False, \n",
    "                  pc_outName can be in value. If a different file type is requested,the user will get error.\n",
    "    \n",
    "    Returns:\n",
    "        pointcloud_pipeline (dict): Dictionary of processing stages in sequential order that define PDAL pipeline.\n",
    "\n",
    "    Raises: \n",
    "        Exception: If user passes in argument that is not 'las' or 'laz'.\n",
    "    \"\"\"\n",
    "    \n",
    "    #this is the basic pipeline which only accesses the 3DEP data\n",
    "    readers = []\n",
    "    for name in usgs_3dep_dataset_names:\n",
    "        url = \"https://s3-us-west-2.amazonaws.com/usgs-lidar-public/{}/ept.json\".format(name)\n",
    "        reader = {\n",
    "            \"type\": \"readers.ept\",\n",
    "            \"filename\": str(url),\n",
    "            \"polygon\": str(extent_epsg3857),\n",
    "            \"requests\": 3,\n",
    "            \"resolution\": pc_resolution\n",
    "        }\n",
    "        readers.append(reader)\n",
    "        \n",
    "    pointcloud_pipeline = {\n",
    "            \"pipeline\":\n",
    "                readers\n",
    "    }\n",
    "    \n",
    "    if filterNoise == True:\n",
    "        \n",
    "        filter_stage = {\n",
    "            \"type\":\"filters.range\",\n",
    "            \"limits\":\"Classification![7:7], Classification![18:18]\"\n",
    "        }\n",
    "        \n",
    "        pointcloud_pipeline['pipeline'].append(filter_stage)\n",
    "    \n",
    "    if reclassify == True:\n",
    "        \n",
    "        remove_classes_stage = {\n",
    "            \"type\":\"filters.assign\",\n",
    "            \"value\":\"Classification = 0\"\n",
    "        }\n",
    "        \n",
    "        classify_ground_stage = {\n",
    "            \"type\":\"filters.smrf\"\n",
    "        }\n",
    "        \n",
    "        reclass_stage = {\n",
    "            \"type\":\"filters.range\",\n",
    "            \"limits\":\"Classification[2:2]\"\n",
    "        }\n",
    "\n",
    "        pointcloud_pipeline['pipeline'].append(remove_classes_stage)\n",
    "        pointcloud_pipeline['pipeline'].append(classify_ground_stage)\n",
    "        pointcloud_pipeline['pipeline'].append(reclass_stage)\n",
    "        \n",
    "    reprojection_stage = {\n",
    "        \"type\":\"filters.reprojection\",\n",
    "        \"out_srs\":\"EPSG:{}\".format(outCRS)\n",
    "    }\n",
    "    \n",
    "    pointcloud_pipeline['pipeline'].append(reprojection_stage)\n",
    "    \n",
    "    if savePointCloud == True:\n",
    "        \n",
    "        if pc_outType == 'las':\n",
    "            savePC_stage = {\n",
    "                \"type\": \"writers.las\",\n",
    "                \"filename\": str(pc_outName)+'.'+ str(pc_outType),\n",
    "            }\n",
    "        elif pc_outType == 'laz':    \n",
    "            savePC_stage = {\n",
    "                \"type\": \"writers.las\",\n",
    "                \"compression\": \"laszip\",\n",
    "                \"filename\": str(pc_outName)+'.'+ str(pc_outType),\n",
    "            }\n",
    "        else:\n",
    "            raise Exception(\"pc_outType must be 'las' or 'laz'.\")\n",
    "\n",
    "        pointcloud_pipeline['pipeline'].append(savePC_stage)\n",
    "        \n",
    "    return pointcloud_pipeline\n",
    "\n",
    "def make_DEM_pipeline(extent_epsg3857, usgs_3dep_dataset_name, pc_resolution, dem_resolution,\n",
    "                      filterNoise = True, reclassify = False, savePointCloud = False, outCRS = 3857,\n",
    "                      pc_outName = 'filter_test', pc_outType = 'laz', demType = 'dtm', gridMethod = 'idw', \n",
    "                      dem_outName = 'dem_test', dem_outExt = 'tif', driver = \"GTiff\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    Build pdal pipeline for creating a digital elevation model (DEM) product from the requested point cloud data. The \n",
    "    user must specify whether a digital terrain (bare earth) model (DTM) or digital surface model (DSM) will be created, \n",
    "    the output DTM/DSM resolution, and the gridding method desired. \n",
    "\n",
    "    The `build_pdal_pipeline() method is used to request the data from the Amazon Web Services ept bucket, and the \n",
    "    user may define any processing steps (filtering, reclassifying, reprojecting). The user must also specify whether \n",
    "    the point cloud should be saved or not. Saving the point cloud is not necessary for the generation of the DEM. \n",
    "\n",
    "    Parameters:\n",
    "        extent_epsg3857 (shapely polygon): User-defined AOI in Web Mercator projection (EPS:3857). Polygon is generated \n",
    "                                           either through the 'handle_draw' methor or by inputing their own shapefile.\n",
    "                                           This parameter is set automatically when the user-defined AOI is chosen.\n",
    "        usgs_3dep_dataset_names (list): List of name of the 3DEP dataset(s) that the data will be obtained. This parameter is set \n",
    "                                        determined through intersecttino of the 3DEP and AOI polys.\n",
    "        pc_resolution (float): The desired resolution of the pointcloud based on the following definition:\n",
    "\n",
    "                        Source: https://pdal.io/stages/readers.ept.html#readers-ept\n",
    "                            A point resolution limit to select, expressed as a grid cell edge length. \n",
    "                            Units correspond to resource coordinate system units. For example, \n",
    "                            for a coordinate system expressed in meters, a resolution value of 0.1 \n",
    "                            will select points up to a ground resolution of 100 points per square meter.\n",
    "                            The resulting resolution may not be exactly this value: the minimum possible \n",
    "                            resolution that is at least as precise as the requested resolution will be selected. \n",
    "                            Therefore the result may be a bit more precise than requested.\n",
    "\n",
    "        pc_outName (str): Desired name of file on user's local file system. If savePointcloud = False, \n",
    "                          pc_outName can be in value.\n",
    "        pc_outType (str): Desired file extension. Input must be either 'las' or 'laz'. If savePointcloud = False, \n",
    "                          pc_outName can be in value. If a different file type is requested,the user will get error.\n",
    "    \n",
    "        dem_resolution (float): Desired grid size (in meters) for output raster DEM \n",
    "        filterNoise (bool): Option to remove points from USGS Class 7 (Low Noise) and Class 18 (High Noise).\n",
    "        reclassify (bool): Option to remove USGS classes and run SMRF to classify ground points only. Default == False.\n",
    "        savePointCloud (bool): Option to save (or not) the point cloud data. If savePointCloud == False, the pc_outName \n",
    "                               and pc_outType parameters are not used and can be any value.\n",
    "\n",
    "        outCRS (int): Output coordinate reference systemt (CRS), specified by ESPG code (e.g., 3857 - Web Mercator)\n",
    "        pc_outName (str): Desired name of file on user's local file system. If savePointcloud = False, \n",
    "                          pc_outName can be in value.\n",
    "        pc_outType (str): Desired file extension. Input must be either 'las' or 'laz'. If a different file type is requested,\n",
    "                    the user will get error stating \"Extension must be 'las' or 'laz'\". If savePointcloud = False, \n",
    "                    pc_outName can be in value.\n",
    "        demType (str): Type of DEM produced. Input must 'dtm' (digital terrain model) or 'dsm' (digital surface model).\n",
    "        gridMethod (str): Method used. Options are 'min', 'mean', 'max', 'idw'.\n",
    "        dem_outName (str): Desired name of DEM file on user's local file system.\n",
    "        dem_outExt (str): DEM file extension. Default is TIF.\n",
    "        driver (str): File format. Default is GTIFF\n",
    "    \n",
    "    Returns:\n",
    "        dem_pipeline (dict): Dictionary of processing stages in sequential order that define PDAL pipeline.\n",
    "    Raises: \n",
    "        Exception: If user passes in argument that is not 'las' or 'laz'.\n",
    "        Exception: If user passes in argument that is not 'dtm' or 'dsm'\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    dem_pipeline = build_pdal_pipeline(extent_epsg3857, usgs_3dep_dataset_name, pc_resolution,\n",
    "                                              filterNoise, reclassify, savePointCloud, outCRS, pc_outName, pc_outType)\n",
    "    \n",
    "    if demType == 'dsm':\n",
    "        dem_stage = {\n",
    "                \"type\":\"writers.gdal\",\n",
    "                \"filename\":str(dem_outName)+ '.' + str(dem_outExt),\n",
    "                \"gdaldriver\":driver,\n",
    "                \"nodata\":-9999,\n",
    "                \"output_type\":gridMethod,\n",
    "                \"resolution\":float(dem_resolution),\n",
    "                \"gdalopts\":\"COMPRESS=LZW,TILED=YES,blockxsize=256,blockysize=256,COPY_SRC_OVERVIEWS=YES\"\n",
    "        }\n",
    "    \n",
    "    elif demType == 'dtm':\n",
    "        groundfilter_stage = {\n",
    "                \"type\":\"filters.range\",\n",
    "                \"limits\":\"Classification[2:2]\"\n",
    "        }\n",
    "\n",
    "        dem_pipeline['pipeline'].append(groundfilter_stage)\n",
    "\n",
    "        dem_stage = {\n",
    "                \"type\":\"writers.gdal\",\n",
    "                \"filename\":str(dem_outName)+ '.' + str(dem_outExt),\n",
    "                \"gdaldriver\":driver,\n",
    "                \"nodata\":-9999,\n",
    "                \"output_type\":gridMethod,\n",
    "                \"resolution\":float(dem_resolution),\n",
    "                \"gdalopts\":\"COMPRESS=LZW,TILED=YES,blockxsize=256,blockysize=256,COPY_SRC_OVERVIEWS=YES\"\n",
    "        }\n",
    "    \n",
    "    else:\n",
    "        raise Exception(\"demType must be 'dsm' or 'dtm'.\")\n",
    "        \n",
    "    dem_pipeline['pipeline'].append(dem_stage)\n",
    "    \n",
    "    return dem_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066165fb",
   "metadata": {},
   "source": [
    "<a name=\"Data-Access-and-Processing\"></a>\n",
    "## Data Access and Processing\n",
    "Now that we have the required modules imported and functions defined, we can proceed with defining our area of interest (AOI), accessing/processing the 3DEP data from the Amazon Web Services EPT bucket. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ecbeb2",
   "metadata": {},
   "source": [
    "### Get 3DEP Dataset Boundary Polygons  \n",
    "Boundaries of the 3DEP dataset are stored as a geojson file on the USGS LIDAR GitHub repo. This repository includes a copy of the file from 2024. For a more up-to-date version, visit https://github.com/hobuinc/usgs-lidar/. \n",
    "\n",
    "For this notebook, we load the GeoJSON file into a geopandas dataframe to find intersection with our area of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdad06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file('../data/shapefiles/resources.geojson').set_crs(4326)\n",
    "gdf_webmercator = gdf.to_crs(3857)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a267308",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('../data/shapefiles/resources.geojson', 'r') as f:\n",
    "    df = gpd.read_file(f)\n",
    "    names = df['name']\n",
    "    urls = df['url']\n",
    "    num_points = df['count']\n",
    "\n",
    "#project the boundaries to EPSG 3857 (necessary for API call to AWS for 3DEP data)\n",
    "projected_geoms = []\n",
    "for geometry in df['geometry']:\n",
    "        projected_geoms.append(gcs_to_proj(geometry))\n",
    "\n",
    "geometries_GCS = df['geometry']\n",
    "geometries_EPSG3857 = gpd.GeoSeries(projected_geoms)\n",
    "\n",
    "print('Done. 3DEP polygons downloaded and projected to Web Mercator (EPSG:3857)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d7030e",
   "metadata": {},
   "source": [
    "Next, we will derive polygons from the Worldview imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353ed20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = Path('../data/baker_csm_stack/original_rasters')\n",
    "left_ortho = datapath / 'final_ortho_left_1.0m_holes_filled.tif'\n",
    "right_ortho = datapath / 'final_ortho_right_1.0m_holes_filled.tif'\n",
    "\n",
    "# read bounds and create corresponding shapes\n",
    "with rasterio.open(left_ortho) as ds:\n",
    "    left_bounds = ds.bounds\n",
    "    left_crs = ds.crs\n",
    "\n",
    "with rasterio.open(right_ortho) as ds:\n",
    "    right_bounds = ds.bounds\n",
    "    right_crs = ds.crs\n",
    "\n",
    "assert left_crs == right_crs, \"CRS of input files must be the same!\"\n",
    "\n",
    "polygon1 = Polygon.from_bounds(*left_bounds)\n",
    "polygon2 = Polygon.from_bounds(*right_bounds)\n",
    "\n",
    "user_aoi = polygon1.intersection(polygon2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572e5899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need the bounds of the polygon to be in EPST 3857. Let's load the geometry into a dataframe and do the conversion\n",
    "user_aoi_gdf = gpd.GeoDataFrame({'geometry':[user_aoi]}, crs=right_crs).to_crs(3857)\n",
    "user_aoi = user_aoi_gdf.geometry.union_all() # dissolves geometry column into a single polygon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6cd5c8",
   "metadata": {},
   "source": [
    "### Find 3DEP Polygon(s) Intersecting AOI\n",
    "Let us now find which 3DEP data intersect the AOI given by the Worldview images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef53c52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersecting_polys = []\n",
    "\n",
    "for i, geom in enumerate(geometries_EPSG3857):\n",
    "    if geom.intersects(user_aoi):\n",
    "        intersecting_polys.append((names[i], geometries_GCS[i], geometries_EPSG3857[i], urls[i], num_points[i]))\n",
    "        \n",
    "print(intersecting_polys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dafa59d",
   "metadata": {},
   "source": [
    "<a name=\"Specify-Point-Cloud-Resolution\"></a>\n",
    "### Specify Point Cloud Resolution\n",
    "Executing the next cell will show the AOI, the relevant 3DEP dataset(s) on another interactive map, and the option to specify point cloud resolution. \n",
    "\n",
    "Importantly, after the map is rendered, the user must define the desired 'point cloud resolution' using the radio buttons below the map. An estimation of the total number of lidar points within the bounding box is provided based on the area of the AOI and the total number of lidar points in the 3DEP dataset(s). Selecting the \"Full\" option will return all points in the quad (this number can be quite large, depending on the size of the AOI). Selecting any of the other options for resolution will return the appropriate number of points to ensure at least one lidar point per Nth meter (where N is the chosen resolution). The Entwine Point Tile (EPT) file format utilizes an octree structure for the point cloud, and the chosen resolution defines how deep in the octree to request points to obtain the specified resolution. This depth, and total number points varies drastically based on a number of parameters including local topography and vegetation. Therefore, the 'resolution' paramater and the total point count do not scale linearly. In other words, specifying a resolution of 2m will likely return far less than half of the number of points returned with 'full' resolution. The estimate of the full poin total provided is not exact, but should give the user some idea of how many points to expect the resultant point cloud to contain. \n",
    "\n",
    "Select the appropriate radio button below the map to specify `pointcloud_resolution`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff54758f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find AOI center for plotting purposes\n",
    "centroid =  user_aoi.centroid\n",
    "\n",
    "#make ipyleaflet map\n",
    "m = ipyleaflet.Map(\n",
    "    basemap=ipyleaflet.basemaps.Esri.WorldTopoMap,\n",
    "    center=(centroid.y, centroid.x),\n",
    "    zoom=12,\n",
    "    )\n",
    "\n",
    "#add intersecting 3DEP polygon(s) to the map\n",
    "wlayer_3DEP_list = []\n",
    "usgs_3dep_datasets = []\n",
    "number_pts_est = []\n",
    "for i, poly in enumerate(intersecting_polys):\n",
    "    wlayer_3DEP = ipyleaflet.WKTLayer(\n",
    "        wkt_string=poly[1].wkt, \n",
    "        style={\"color\": \"green\"})\n",
    "    \n",
    "    m.add_layer(wlayer_3DEP)\n",
    "    wlayer_3DEP_list.append(wlayer_3DEP)\n",
    "    usgs_3dep_datasets.append(poly[0])\n",
    "    \n",
    "    #estimate total points using ratio of area and point count\n",
    "    number_pts_est.append((int((user_aoi.area/poly[2].area)*(poly[4]))))\n",
    "\n",
    "\n",
    "# #make ipyleaflet layers from the AOI and add to map\n",
    "# wlayer_user = ipyleaflet.WKTLayer(\n",
    "#     wkt_string=AOI_GCS.boundary.wkt,\n",
    "#     style={\"color\": \"blue\"}\n",
    "# )\n",
    "\n",
    "# AOI_EPSG3857_wkt = AOI_EPSG3857.wkt\n",
    "# m.add_layer(wlayer_user)\n",
    "\n",
    "\n",
    "#sum the estimates of the number of points from each 3DEP dataset within the AOI\n",
    "num_pts_est = sum(number_pts_est)\n",
    "\n",
    "#Plot map and specify desired point cloud resolution using a widget\n",
    "user_resolution = widgets.RadioButtons(\n",
    "    options=[\n",
    "        (f'Full - All ~{int(math.ceil(num_pts_est/1e6)*1e6):,} points', 1.0),\n",
    "        (f'High - 1m resolution', 1.0),\n",
    "        (f'Mid  - 5m resolution', 5.0),\n",
    "        (f'Low  - 10m resolution', 10.0)\n",
    "    ],\n",
    "    layout={'width': 'max-content'},\n",
    "    disabled = False,\n",
    ")\n",
    "\n",
    "display(m)\n",
    "print(f'Your AOI at full resolution will include approximately {int(math.ceil(num_pts_est/1e6)*1e6):,} points. Select desired point cloud resolution.')\n",
    "widgets.VBox(\n",
    "    [user_resolution]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dc32e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ba85938",
   "metadata": {},
   "source": [
    "<font color='red'>**Note**: Lidar point clouds can get *very* large, *very* fast, and the point counts provided above are an estimate. Selecting the `Full` or `High` option when using Google Colaboratory (12GB RAM allocation) may cause the runtime to fail when attempting to access data exceeding ~50-100 million points. If full resolution is desired, we recommend running this notebook locally on hardware with more RAM. Keep this in mind when deciding the AOI size and point cloud resolution above!</font>\n",
    "\n",
    "The AOI bounding box, the relevant 3DEP dataset name(s), and the desired point cloud resolution are now defined. We can proceed with the API request to the AWS EPT bucket, processing, visualizing, and saving the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f17c46",
   "metadata": {},
   "source": [
    "<a name=\"Construct-and-Exectute-PDAL-Pipeline-for-Point-Cloud-Data\"></a>\n",
    "### Construct and Exectute PDAL Pipeline for Point Cloud Data\n",
    "The Point Data Abstraction Library (PDAL) is an open-source tool for translating and manipulating point cloud data. PDAL pipelines are useful ways of processing and manipulating point cloud data and creating derivative products. Pipelines comprise one or more stages that are read and executed in order on the point cloud dataset(s). \n",
    "\n",
    "The PDAL pipeline is constructed using the ```build_pdal_pipeline()``` function, and will construct the appropriate pipeline for the user's specifications (defined as function arguments). Executing this pipeline will make the API request, perform processing on the point cloud data (chosen by user) and provide the final result on the user's file system of Google Drive (Google Colab).  \n",
    "\n",
    "Paramaters (for more detailed descriptions of parameters, see <a href=\"#Define-Functions\" data-toc-modified-id=\"Define-Functions-6.3\">the function definitions</a>, above: <br>\n",
    "```AOI_EPSG3857_wkt```: the user-defined area of interest (AOI)<br>\n",
    "```usgs_3dep_datasets```: the intersecting 3DEP dataset names<br>\n",
    "```pointcloud_resolution```: point cloud resolution (1m, 2m, 5m, 10m)<br>\n",
    "```filterNoise```: remove the points of Class 7 (low noise) and Class 18 (high noise);<br>\n",
    "```reclassify```: remove USGS classes and run an SMRF to classify ground points only<br>\n",
    "```savePointCloud```: specify if point cloud data should be saved to local file system<br>\n",
    "```outCRS```: specify the coordinate reference system (CRS, in EPSG) of the output dataset.<br>\n",
    "```pc_outName```: name of point cloud on local file system<br>\n",
    "```pc_outType```: file type, las or laz (laszip compression). Options are 'las' or 'laz'<br>\n",
    "\n",
    "**Important Note 1: The ```AOI_EPSG3857_wkt```, ```usgs_3dep_datasets```, and ```pointcloud_resolution``` arguments are already defined. These should not be modified.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1555ea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify AOI_EPSG3857_wkt, usgs_3dep_datasets, or pointcloud_resolution\n",
    "# Modify the optional arguments to fit user need.\n",
    "# Change outCRS to EPSG code of desired coordinate reference system (Default is EPSG:3857 - Web Mercator Projection)\n",
    "# Change pc_outname to descriptive name and pc_outType to 'las' or 'laz'.\n",
    "\n",
    "pointcloud_resolution = user_resolution.value\n",
    "pc_pipeline = build_pdal_pipeline(AOI_EPSG3857_wkt, usgs_3dep_datasets, pointcloud_resolution, filterNoise = True,\n",
    "                                  reclassify = False, savePointCloud = True, outCRS = 3857, \n",
    "                                  pc_outName = 'pointcloud_test', pc_outType = 'laz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c40315",
   "metadata": {},
   "source": [
    "The PDAL pipeline is now constructed. Running the the PDAL Python bindings function ```pdal.Pipeline()``` creates the pdal.Pipeline object from a json-ized version of the pointcloud pipeline we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e0dbd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pc_pipeline = pdal.Pipeline(json.dumps(pc_pipeline))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5304a7ef",
   "metadata": {},
   "source": [
    "The cell below will execute the pc_pipeline object, which will make the API request, performing processing, and save the point cloud (if `savePointCloud == True`) at the specified location, name, and extension.\n",
    "\n",
    "Executing the pipeline in streaming mode will speed up the process and cuts down on the required RAM. The `%%time` magic command will return the total computation time. The final output is the total number of points returned.\n",
    "\n",
    "**Note**: If `reclassify == True` in the pipeline constructed above, a step is added for removing assigned USGS classifications and running a SMRF filter to classify ground points only. When `reclassify == True`, the PDAL pipeline cannot be executed in streaming mode, as reclassification requires all points to be present in memory. **<font color='red'>Be aware that this will be slower than executing in streaming mode and may not be possible for very large point clouds due to RAM limitations.</font>** Commands for executing the pipeline in streaming and non-streaming mode are included below. Comment/uncomment the appropriate command below (depending on whether `reclassify == True` or `reclassify == False` in the pipeline constructed above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0d7cb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "pc_pipeline.execute_streaming(chunk_size=1000000) # use this if reclassify == False \n",
    "#pc_pipeline.execute() # use this if reclassify == True "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da107b41",
   "metadata": {},
   "source": [
    "If the user only desires point cloud data, they may stop here. Following is an overview on how a DSM and DTM may be created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2ed7a5",
   "metadata": {},
   "source": [
    "<a name=\"Digital-Elevation-Model-(DEM)-Generation\"></a>\n",
    "### Digital Elevation Model (DEM) Generation \n",
    "The following cells may be run to produce a DSM and DTM of the user-defined AOI. The DTM is produced using only points classified as 'Ground' (USGS Class 2), and therefore, represents the surface of the ground beneath any vegetation. Alternatively, a DSM uses all points to produce a representation of the surface including vegetation and other structures. \n",
    "\n",
    "The DTM/DSM is produced using an analogous approach to point cloud request and processsing. Namely, a PDAL pipeline is constructed using the function `make_DEM_pipeline()`. This function first constructs a pipeline using `build_pdal_pipeline()` to create stages related to point cloud access and processing. The function then appends PDAL stages specific to the creation of gridded products that fit the specification of the user (defined as function arguments), namely dem resolution and dem type.\n",
    "\n",
    "Paramaters (for more detailed descriptions of parameters, see <a href=\"#Define-Functions\" data-toc-modified-id=\"Define-Functions-6.3\">the function definitions</a>, above:<br>\n",
    "`AOI_EPSG3857_wkt`: the user-defined area of interest (AOI)<br>\n",
    "`usgs_3dep_dataset`: the corresponding 3DEP dataset name<br>\n",
    "`pointcloud_resolution`: point cloud resolution (1m, 2m, 5m, 10m)<br>\n",
    "`dem_generation`: grid size for dem product (specified in meters)<br>\n",
    "`filterNoise`: remove the points of Class 7 (noise); optional<br>\n",
    "`reclassify`: remove USGS classes and run an SMRF to classify ground points only<br>\n",
    "`savePointCloud`: specify if point cloud data should be saved to local file system<br>\n",
    "`pc_outName`: name of point cloud on local file system<br>\n",
    "`pc_outType`: file type, las or laz (laszip compression). Options are 'las' or 'laz'<br>\n",
    "`demType`: specifies to create digital surface model (DSM) or digital terrain model (DTM)<br>\n",
    "`gridMethod`: gridding method to use; options: (min, mean, max, idw)<br>\n",
    "`dem_outName`: name of dem on local file system <br>\n",
    "`dem_outExt`: extension of file on local file system (must correspond to what is chosen for ```driver```<br>\n",
    "`driver`: gdal code of the driver (default is \"GTiff\"; other options can be found at https://gdal.org/drivers/raster/index.html<br>\n",
    "\n",
    "\n",
    "**Important note 1: The `make_DEM_pipeline()` function is used for the making of both DSM and DTM products. The type of DEM (DSM/DTM) is specified in the `demType` argument (e.g., `demType = 'dsm'`)**\n",
    "\n",
    "**Important Note 2: The `AOI_EPSG3857_wkt`, `usgs_3dep_datasets`, and `pointcloud_resolution` arguments are already defined after running the above cells. These should not be modified.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2564000a",
   "metadata": {},
   "source": [
    "<a name=\"Make-Digital-Surface-Model-(DSM)\"></a>\n",
    "### Make Digital Surface Model (DSM)\n",
    "The following cells will produce a Digital Surface Model (DSM) using all of the lidar returns in the point cloud.\n",
    "Do not modify the `AOI_EPSG3857_wkt`, `usgs_3dep_datasets`, or `pointcloud_resolution` arguments. Specify the desired dsm resolution (in meters), the appropriate point cloud processing steps, and the file names/extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e23c3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Do not modify AOI_EPSG3857_wkt, usgs_3dep_datasets, or pointcloud_resolution\n",
    "# Modify the optional arguments to fit user need.\n",
    "# Change outCRS to EPSG code of desired coordinate reference system (Default is EPSG:3857 - Web Mercator Projection)\n",
    "# Change dem_outName to descriptive name; dem_outExt can be any extension supported by gdal.\n",
    "\n",
    "pointcloud_resolution = user_resolution.value\n",
    "dsm_resolution = 1.0\n",
    "dsm_pipeline = make_DEM_pipeline(AOI_EPSG3857_wkt, usgs_3dep_datasets, pointcloud_resolution, dsm_resolution,\n",
    "                                 filterNoise = True, reclassify = False,  savePointCloud = False, outCRS = 32613,\n",
    "                                 pc_outName = 'pointcloud_test', pc_outType = 'laz', demType = 'dsm', \n",
    "                                 gridMethod='idw', dem_outName = 'test_dsm', dem_outExt = 'tif', driver = \"GTiff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a42293",
   "metadata": {},
   "source": [
    "The PDAL pipeline is now constructed for making the DSM. Running the the PDAL Python bindings function ```pdal.Pipeline()``` creates the pdal.Pipeline object from a json-ized version of the pointcloud pipeline we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435fb31a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dsm_pipeline = pdal.Pipeline(json.dumps(dsm_pipeline))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbc9d93",
   "metadata": {},
   "source": [
    "The cell below will execute the dsm_pipeline object, which will make the API request, performing processing, and save the point cloud (if `savePointCloud == True`) and create and save the DSM at the specified location, name, and extension. \n",
    "\n",
    "Executing the pipeline in streaming mode will speed up the process and cuts down on the required RAM. The `%%time` magic command will return the total computation time. The final output is the total number of points returned.\n",
    "\n",
    "**Note**: If `reclassify == True` in the pipeline constructed above, a step is added for removing assigned USGS classifications and running a SMRF filter to classify ground points only. When `reclassify == True`, the PDAL pipeline cannot be executed in streaming mode, as reclassification requires all points to be present in memory. **<font color='red'>Be aware that this will be slower than executing in streaming mode and may not be possible for very large point clouds due to RAM limitations.</font>** Commands for executing the pipeline in streaming and non-streaming mode are included below. Comment/uncomment the appropriate command below (depending on whether `reclassify == True` or `reclassify == False` in the pipeline constructed above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f749cf45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "dsm_pipeline.execute_streaming(chunk_size=1000000) # use this if reclassify == False\n",
    "#dsm_pipeline.execute() # use this if reclassify == True  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2038adf",
   "metadata": {},
   "source": [
    "Below, the same process is outlined for the making of at DTM. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcd9898",
   "metadata": {},
   "source": [
    "<a name=\"Visualize-DEMs-(DSM/DTM)\"></a>\n",
    "## Visualize the DEM (DSM/DTM)\n",
    "We can now visualize the DSM or DTM products in the Jupyter Notebook. We use the <a href=\"https://corteva.github.io/rioxarray/stable/\"> rioxarray </a> and <a href=\"https://matplotlib.org/stable/users/index.html\"> matplotlib </a> Python libraries for simple plotting. We import `rioxarray` and `matplotlib.pyplot` here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a4da99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rioxarray as rio\n",
    "from rasterio.enums import Resampling\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc001fe",
   "metadata": {},
   "source": [
    "Now we must define the file name we would like to plot. This could be a file path (e.g., `/path/to/my/dtm/dtm.tif`). Then we open the dtm as an `xarray` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e1e002",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_name = 'test_dtm.tif' # /path/to/your/dtm/dtm.tif\n",
    "dtm = rio.open_rasterio(dtm_name, masked=True).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83093b8",
   "metadata": {},
   "source": [
    "DEMs can be very large and require significant RAM to plot. Here, we apply a downsampling technique for more efficient visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff9b277",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = downsample_dem(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63db4058",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "dtm.plot(cmap=\"viridis\", robust=True)\n",
    "plt.title(\"Digital Terrain Model (DTM) in Meters\")\n",
    "plt.ticklabel_format(style=\"plain\")\n",
    "plt.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf65c16",
   "metadata": {},
   "source": [
    "<a name=\"Resources\"></a>\n",
    "## Resources\n",
    "\n",
    "All OpenTopography USGS 3DEP scientific workflows in this collection:<br>\n",
    "\n",
    "1. [Generate and visualize DEMs (DTM and DSM) from USGS 3D Elevation Program (3DEP) lidar data for user-defined area of interest](https://github.com/OpenTopography/OT_3DEP_Workflows/blob/main/notebooks/01_3DEP_Generate_DEM_User_AOI.ipynb)[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/OpenTopography/OT_3DEP_Workflows/blob/main/notebooks/01_3DEP_Generate_DEM_User_AOI.ipynb) <br>\n",
    "\n",
    "2. [Generate and visualize DEMs (DTM and DSM) from USGS 3D Elevation Program (3DEP) lidar data for USGS 7.5’ Quadrangles](https://github.com/OpenTopography/OT_3DEP_Workflows/blob/main/notebooks/02_3DEP_Generate_DEM_USGS_7.5_Quadrangles.ipynb)[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/OpenTopography/OT_3DEP_Workflows/blob/main/notebooks/02_3DEP_Generate_DEM_USGS_7.5_Quadrangles.ipynb) <br>\n",
    "\n",
    "3. [Generate and visualize DEMs (DTM and DSM) from USGS 3D Elevation Program (3DEP) lidar data for USGS Hydrologic Units](https://github.com/OpenTopography/OT_3DEP_Workflows/blob/main/notebooks/03_3DEP_Generate_DEM_USGS_HUCs.ipynb)[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/OpenTopography/OT_3DEP_Workflows/blob/main/notebooks/03_3DEP_Generate_DEM_USGS_HUCs.ipynb) <br>\n",
    "\n",
    "4. [Generate and visualize DEMs (DTM and DSM) from USGS 3D Elevation Program (3DEP) lidar data for user-defined corridors](https://github.com/OpenTopography/OT_3DEP_Workflows/blob/main/notebooks/04_3DEP_Generate_DEM_Corridors.ipynb)[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/OpenTopography/OT_3DEP_Workflows/blob/main/notebooks/04_3DEP_Generate_DEM_Corridors.ipynb) <br>\n",
    "\n",
    "5. [Generate Canopy Height Model (CHM) using USGS 3D Elevation Program (3DEP) lidar data for user-defined area of interest](https://github.com/OpenTopography/OT_3DEP_Workflows/blob/main/notebooks/05_3DEP_Generate_Canopy_Height_Models_User_AOI.ipynb)[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/OpenTopography/OT_3DEP_Workflows/blob/main/notebooks/05_3DEP_Generate_Canopy_Height_Models_User_AOI.ipynb) <br>\n",
    "\n",
    "6. [Topographic Differencing using USGS 3D Elevation Program (3DEP) lidar data for user-defined area of interest](https://github.com/OpenTopography/OT_3DEP_Workflows/blob/main/notebooks/06_3DEP_Topographic_Differencing.ipynb)[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/OpenTopography/OT_3DEP_Workflows/blob/main/notebooks/06_3DEP_Topographic_Differencing.ipynb) <br>\n",
    "\n",
    "7. [Generate colorized (RGB) point clouds using USGS 3D Elevation Program (3DEP) lidar data and National Agriculture Imagery Program (NAIP) Imagery](https://github.com/OpenTopography/OT_3DEP_Workflows/blob/main/notebooks/07_3DEP_Generate_Colorized_PointClouds.ipynb)[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/OpenTopography/OT_3DEP_Workflows/blob/main/notebooks/07_3DEP_Generate_Colorized_PointClouds.ipynb)\n",
    "    \n",
    "### Additional Resources\n",
    "\n",
    "- Access USGS 3DEP via the <a href=\"https://portal.opentopography.org/datasets\">OpenTopography</a> portal (Currently restricted to academics).\n",
    "\n",
    "- The USGS 3DEP Lidar Point Cloud Data are accessible in Entwine Point Tile (EPT) format from this <a href=\"https://registry.opendata.aws/usgs-lidar/\">Amazon Web Services S3 Bucket</a>. <br>\n",
    "\n",
    "- The USGS hydrologic unit boundaries are accessed via the <a href=\"https://hydro.nationalmap.gov/arcgis/rest/services/wbd/MapServer\">USGS Watershed Dataset Map Service</a>. <br>\n",
    "\n",
    "- The USGS 7.5' quadrangle boundaries are accessed via the <a href=\"https://carto.nationalmap.gov/arcgis/rest/services/map_indices/MapServer\"> USGS Map Indicies Service</a>. <br>\n",
    "\n",
    "- Documentation for open-source Python libararies used by these workflows include <a href=\"https://pdal.dev/en/latest/\">PDAL</a> and <a href=\"https://gdal.org/\">GDAL</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
