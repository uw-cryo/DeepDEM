{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dda9bb9",
   "metadata": {},
   "source": [
    "# Generate DSM from USGS 3D Elevation Program (3DEP) lidar data corresponding to Worldview Mount Baker imagery to train DeepDEM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f1a034",
   "metadata": {},
   "source": [
    "This notebook has been repurposed from the OpenTopography GitHub repository [here](https://github.com/OpenTopography/OT_3DEP_Workflows/blob/main/notebooks/01_3DEP_Generate_DEM_User_AOI.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9692c43b",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "This notebook enables users to download USGS LIDAR data from Amazon Web Services (AWS) S3 public bucket and process them for use with DeepDEM as training data.\n",
    "\n",
    "#### Specific features of this notebook\n",
    "\n",
    "1. The notebook reads in the bounds of the Worldview imagery over which we would like to generate a DSM\n",
    "\n",
    "2. Send an API request to <a href=\"https://registry.opendata.aws/usgs-lidar/\"> Amazon Web Services (AWS) EPT (Entwine Point Tile) S3 bucket</a> returns 3DEP point cloud data within user-defined AOI. \n",
    "\n",
    "3. Process returned pointcloud using PDAL.\n",
    "\n",
    "4. Create a Digital Surface Model (DSM) with user-specifed resolution, gridding method, and file type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7513dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data parsing imports\n",
    "import json\n",
    "\n",
    "# GIS imports\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "import rasterio\n",
    "\n",
    "# PDAL imports\n",
    "import pdal\n",
    "\n",
    "# Misc imports\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d924c977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pdal_pipeline(extent_polygon, usgs_3dep_dataset_names, pc_resolution, filterNoise = False,\n",
    "                        reclassify = False, savePointCloud = True, outCRS = 3857, pc_outName = 'filter_test', \n",
    "                        pc_outType = 'laz'):\n",
    "\n",
    "    \"\"\"\n",
    "    This method builds a PDAL pipeline for requesting, processing, and saving point cloud data. Each processing step is a 'stage' \n",
    "    in the final pdal pipeline. Each stages is appended to the 'pointcloud_pipeline' object to produce the final pipeline.\n",
    "    \n",
    "    Parameters:\n",
    "    extent_polygon (shapely polygon): Polygon for user-defined AOI in Web Mercator projection (EPS:3857)\n",
    "    usgs_3dep_dataset_names (str): List of name of the 3DEP dataset(s) that the data will be obtained.\n",
    "    pc_resolution (float): The desired resolution of the pointcloud\n",
    "                            \n",
    "    filterNoise (bool): Option to remove points from USGS Class 7 (Low Noise) and Class 18 (High Noise).\n",
    "    reclassify (bool): Option to remove USGS classes and run SMRF to classify ground points only. Default == False.\n",
    "    savePointCloud (bool): Option to save (or not) the point cloud data. If savePointCloud == False, \n",
    "           the pc_outName and pc_outType parameters are not used and can be any value.\n",
    "    outCRS (int): Output coordinate reference systemt (CRS), specified by ESPG code (e.g., 3857 - Web Mercator)\n",
    "    pc_outName (str): Desired name of file on user's local file system. If savePointcloud = False, \n",
    "                  pc_outName can be in value.\n",
    "    pc_outType (str):  Desired file extension. Input must be either 'las' or 'laz'. If savePointcloud = False, \n",
    "                  pc_outName can be in value. If a different file type is requested,the user will get error.\n",
    "    \n",
    "    Returns:\n",
    "        pointcloud_pipeline (dict): Dictionary of processing stages in sequential order that define PDAL pipeline.\n",
    "\n",
    "    Raises: \n",
    "        Exception: If user passes in argument that is not 'las' or 'laz'.\n",
    "    \"\"\"\n",
    "    \n",
    "    #this is the basic pipeline which only accesses the 3DEP data\n",
    "    readers = []\n",
    "    for name in usgs_3dep_dataset_names:\n",
    "        url = \"https://s3-us-west-2.amazonaws.com/usgs-lidar-public/{}/ept.json\".format(name)\n",
    "        reader = {\n",
    "            \"type\": \"readers.ept\",\n",
    "            \"filename\": str(url),\n",
    "            \"polygon\": str(extent_polygon),\n",
    "            \"requests\": 3,\n",
    "            \"resolution\": pc_resolution\n",
    "        }\n",
    "        readers.append(reader)\n",
    "        \n",
    "    pointcloud_pipeline = {\n",
    "            \"pipeline\":\n",
    "                readers\n",
    "    }\n",
    "    \n",
    "    if filterNoise == True:\n",
    "        \n",
    "        # Class 7 and 18 correspond to \"Low Noise\" and \"High Noise\". See USGS document \"Lidar Base Specification, Techniques and Methods\"\n",
    "        # These data will be filtered out when processing the point cloud\n",
    "\n",
    "        filter_stage = {\n",
    "            \"type\":\"filters.range\",\n",
    "            \"limits\":\"Classification![7:7], Classification![18:18]\" \n",
    "        }\n",
    "        \n",
    "        pointcloud_pipeline['pipeline'].append(filter_stage)\n",
    "    \n",
    "    if reclassify == True:\n",
    "        \n",
    "        remove_classes_stage = {\n",
    "            \"type\":\"filters.assign\",\n",
    "            \"value\":\"Classification = 0\"\n",
    "        }\n",
    "        \n",
    "        classify_ground_stage = {\n",
    "            \"type\":\"filters.smrf\"\n",
    "        }\n",
    "        \n",
    "        reclass_stage = {\n",
    "            \"type\":\"filters.range\",\n",
    "            \"limits\":\"Classification[2:2]\"\n",
    "        }\n",
    "\n",
    "        pointcloud_pipeline['pipeline'].append(remove_classes_stage)\n",
    "        pointcloud_pipeline['pipeline'].append(classify_ground_stage)\n",
    "        pointcloud_pipeline['pipeline'].append(reclass_stage)\n",
    "        \n",
    "    reprojection_stage = {\n",
    "        \"type\":\"filters.reprojection\",\n",
    "        \"out_srs\":\"EPSG:{}\".format(outCRS)\n",
    "    }\n",
    "    \n",
    "    pointcloud_pipeline['pipeline'].append(reprojection_stage)\n",
    "    \n",
    "    if savePointCloud == True:\n",
    "        \n",
    "        if pc_outType == 'las':\n",
    "            savePC_stage = {\n",
    "                \"type\": \"writers.las\",\n",
    "                \"filename\": str(pc_outName)+'.'+ str(pc_outType),\n",
    "            }\n",
    "        elif pc_outType == 'laz':    \n",
    "            savePC_stage = {\n",
    "                \"type\": \"writers.las\",\n",
    "                \"compression\": \"laszip\",\n",
    "                \"filename\": str(pc_outName)+'.'+ str(pc_outType),\n",
    "            }\n",
    "        else:\n",
    "            raise Exception(\"pc_outType must be 'las' or 'laz'.\")\n",
    "\n",
    "        pointcloud_pipeline['pipeline'].append(savePC_stage)\n",
    "        \n",
    "    return pointcloud_pipeline\n",
    "\n",
    "def make_DEM_pipeline(extent_polygon, usgs_3dep_dataset_name, pc_resolution, dem_resolution,\n",
    "                      filterNoise = True, reclassify = False, savePointCloud = False, outCRS = 3857,\n",
    "                      pc_outName = 'filter_test', pc_outType = 'laz', demType = 'dtm', gridMethod = 'idw', \n",
    "                      dem_outName = 'dem_test', driver = \"GTiff\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    Build pdal pipeline for creating a digital elevation model (DEM) product from the requested point cloud data. The \n",
    "    user must specify whether a digital terrain (bare earth) model (DTM) or digital surface model (DSM) will be created, \n",
    "    the output DTM/DSM resolution, and the gridding method desired. \n",
    "\n",
    "    The `build_pdal_pipeline() method is used to request the data from the Amazon Web Services ept bucket, and the \n",
    "    user may define any processing steps (filtering, reclassifying, reprojecting). The user must also specify whether \n",
    "    the point cloud should be saved or not. Saving the point cloud is not necessary for the generation of the DEM. \n",
    "\n",
    "    Parameters:\n",
    "        extent_epsg3857 (shapely polygon): User-defined AOI in Web Mercator projection (EPS:3857). Polygon is generated \n",
    "                                           either through the 'handle_draw' methor or by inputing their own shapefile.\n",
    "                                           This parameter is set automatically when the user-defined AOI is chosen.\n",
    "        usgs_3dep_dataset_names (list): List of name of the 3DEP dataset(s) that the data will be obtained. This parameter is set \n",
    "                                        determined through intersecttino of the 3DEP and AOI polys.\n",
    "        pc_resolution (float): The desired resolution of the pointcloud based on the following definition:\n",
    "\n",
    "                        Source: https://pdal.io/stages/readers.ept.html#readers-ept\n",
    "                            A point resolution limit to select, expressed as a grid cell edge length. \n",
    "                            Units correspond to resource coordinate system units. For example, \n",
    "                            for a coordinate system expressed in meters, a resolution value of 0.1 \n",
    "                            will select points up to a ground resolution of 100 points per square meter.\n",
    "                            The resulting resolution may not be exactly this value: the minimum possible \n",
    "                            resolution that is at least as precise as the requested resolution will be selected. \n",
    "                            Therefore the result may be a bit more precise than requested.\n",
    "\n",
    "        pc_outName (str): Desired name of file on user's local file system. If savePointcloud = False, \n",
    "                          pc_outName can be in value.\n",
    "        pc_outType (str): Desired file extension. Input must be either 'las' or 'laz'. If savePointcloud = False, \n",
    "                          pc_outName can be in value. If a different file type is requested,the user will get error.\n",
    "    \n",
    "        dem_resolution (float): Desired grid size (in meters) for output raster DEM \n",
    "        filterNoise (bool): Option to remove points from USGS Class 7 (Low Noise) and Class 18 (High Noise).\n",
    "        reclassify (bool): Option to remove USGS classes and run SMRF to classify ground points only. Default == False.\n",
    "        savePointCloud (bool): Option to save (or not) the point cloud data. If savePointCloud == False, the pc_outName \n",
    "                               and pc_outType parameters are not used and can be any value.\n",
    "\n",
    "        outCRS (int): Output coordinate reference systemt (CRS), specified by ESPG code (e.g., 3857 - Web Mercator)\n",
    "        pc_outName (str): Desired name of file on user's local file system. If savePointcloud = False, \n",
    "                          pc_outName can be in value.\n",
    "        pc_outType (str): Desired file extension. Input must be either 'las' or 'laz'. If a different file type is requested,\n",
    "                    the user will get error stating \"Extension must be 'las' or 'laz'\". If savePointcloud = False, \n",
    "                    pc_outName can be in value.\n",
    "        demType (str): Type of DEM produced. Input must 'dtm' (digital terrain model) or 'dsm' (digital surface model).\n",
    "        gridMethod (str): Method used. Options are 'min', 'mean', 'max', 'idw'.\n",
    "        dem_outName (str): Desired name of DEM file on user's local file system.\n",
    "        driver (str): File format. Default is GTIFF\n",
    "    \n",
    "    Returns:\n",
    "        dem_pipeline (dict): Dictionary of processing stages in sequential order that define PDAL pipeline.\n",
    "    Raises: \n",
    "        Exception: If user passes in argument that is not 'las' or 'laz'.\n",
    "        Exception: If user passes in argument that is not 'dtm' or 'dsm'\n",
    "\n",
    "    \"\"\"\n",
    "    if demType not in [\"dtm\", \"dsm\"]:\n",
    "        raise Exception(\"demType must be either 'dsm' or 'dtm'\")\n",
    "\n",
    "    dem_pipeline = build_pdal_pipeline(extent_polygon, usgs_3dep_dataset_name, pc_resolution,\n",
    "                                              filterNoise, reclassify, savePointCloud, outCRS, pc_outName, pc_outType)\n",
    "\n",
    "    dem_stage = {\n",
    "            \"type\":\"writers.gdal\",\n",
    "            \"filename\":str(dem_outName),\n",
    "            \"gdaldriver\":driver,\n",
    "            \"nodata\":-9999,\n",
    "            \"output_type\":gridMethod,\n",
    "            \"radius\":1,\n",
    "            \"resolution\":float(dem_resolution),\n",
    "            \"gdalopts\":\"COMPRESS=LZW,TILED=YES,blockxsize=256,blockysize=256,COPY_SRC_OVERVIEWS=YES\"\n",
    "    }\n",
    "    \n",
    "    if demType == 'dtm':\n",
    "        groundfilter_stage = {\n",
    "                \"type\":\"filters.range\",\n",
    "                \"limits\":\"Classification[2:2]\"\n",
    "        }\n",
    "\n",
    "        dem_pipeline['pipeline'].append(groundfilter_stage)\n",
    "        \n",
    "    dem_pipeline['pipeline'].append(dem_stage)\n",
    "    \n",
    "    return dem_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066165fb",
   "metadata": {},
   "source": [
    "<a name=\"Data-Access-and-Processing\"></a>\n",
    "## Data Access and Processing\n",
    "Now that we have the required modules imported and functions defined, we can proceed with defining our area of interest (AOI), accessing/processing the 3DEP data from the Amazon Web Services EPT bucket. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ecbeb2",
   "metadata": {},
   "source": [
    "### Get 3DEP Dataset Boundary Polygons  \n",
    "Boundaries of the 3DEP dataset are stored as a geojson file on the USGS LIDAR GitHub repo. This repository includes a copy of the file from 2024. For a more up-to-date version, visit https://github.com/hobuinc/usgs-lidar/. \n",
    "\n",
    "For this notebook, we load the GeoJSON file into a geopandas dataframe to find intersection with our area of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a267308",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('../data/shapefiles/resources.geojson', 'r') as f:\n",
    "    df = gpd.read_file(f)\n",
    "    names = df['name']\n",
    "    urls = df['url']\n",
    "    num_points = df['count']\n",
    "\n",
    "geometries_GCS = df['geometry']\n",
    "df.set_crs(4326)\n",
    "df_3857 = df.to_crs(3857)\n",
    "\n",
    "print('Done. 3DEP polygons downloaded and projected to Web Mercator (EPSG:3857)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d7030e",
   "metadata": {},
   "source": [
    "Next, we will derive polygons from the Worldview imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353ed20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify path to location of ortho images\n",
    "datapath = Path('../data/baker_csm_stack/original_rasters')\n",
    "left_ortho = datapath / 'final_ortho_left_1.0m_holes_filled.tif'\n",
    "right_ortho = datapath / 'final_ortho_right_1.0m_holes_filled.tif'\n",
    "\n",
    "# # read bounds and create corresponding shapes\n",
    "with rasterio.open(left_ortho) as ds:\n",
    "    left_bounds = ds.bounds\n",
    "    left_crs = ds.crs\n",
    "\n",
    "with rasterio.open(right_ortho) as ds:\n",
    "    right_bounds = ds.bounds\n",
    "    right_crs = ds.crs\n",
    "\n",
    "assert left_crs == right_crs, \"CRS of input files must be the same!\"\n",
    "\n",
    "polygon1 = Polygon.from_bounds(*left_bounds)\n",
    "polygon2 = Polygon.from_bounds(*right_bounds)\n",
    "\n",
    "user_aoi = polygon1.intersection(polygon2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572e5899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need the bounds of the polygon to be in EPST 3857. Let's load the geometry into a dataframe and do the conversion\n",
    "user_aoi_gdf = gpd.GeoDataFrame({'geometry':[user_aoi]}, crs=right_crs).to_crs(3857)\n",
    "user_aoi = user_aoi_gdf.geometry.union_all() # dissolves geometry column into a single polygon\n",
    "user_aoi_wkt = user_aoi.wkt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6cd5c8",
   "metadata": {},
   "source": [
    "### Find 3DEP Polygon(s) Intersecting AOI\n",
    "Let us now find which 3DEP data intersect the AOI given by the Worldview images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef53c52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersecting_polys = []\n",
    "\n",
    "for i, geom in enumerate(df_3857.geometry):\n",
    "    if geom.intersects(user_aoi):\n",
    "        intersecting_polys.append((names[i], df.iloc[i].geometry, df_3857.loc[i].geometry, df.iloc[i].url, df.iloc[i].count))\n",
    "        \n",
    "usgs_dataset_names = [x[0] for x in intersecting_polys]\n",
    "print(\"Names of intersecting datasets in the 3DEP database: \", usgs_dataset_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1555ea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify pointcloud resolution needed\n",
    "\n",
    "pointcloud_file =  datapath / 'ot_mtbaker_dsm_pointcloud'\n",
    "pointcloud_resolution = 1.0 # in meters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2564000a",
   "metadata": {},
   "source": [
    "<a name=\"Make-Digital-Surface-Model-(DSM)\"></a>\n",
    "### Make Digital Surface Model (DSM)\n",
    "The following cells will produce a Digital Surface Model (DSM) using all of the lidar returns in the point cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e23c3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dsm_resolution = 1.0\n",
    "dsm_pipeline = make_DEM_pipeline(user_aoi_wkt, usgs_dataset_names, pointcloud_resolution, dsm_resolution,\n",
    "                                 filterNoise = True, reclassify = False,  savePointCloud = False, outCRS = 32610,\n",
    "                                 pc_outName = pointcloud_file, pc_outType = 'laz', demType = 'dsm', \n",
    "                                 gridMethod='idw', dem_outName = datapath / 'ot_mtbaker_dsm.tif', driver = \"GTiff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a42293",
   "metadata": {},
   "source": [
    "The PDAL pipeline is now constructed for making the DSM. Running the the PDAL Python bindings function ```pdal.Pipeline()``` creates the pdal.Pipeline object from a json-ized version of the pointcloud pipeline we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435fb31a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dsm_pipeline = pdal.Pipeline(json.dumps(dsm_pipeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f749cf45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "dsm_pipeline.execute_streaming(chunk_size=1000000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
