{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of the DeepDEM framework for refining DSMs, we need to calculate a global scale factor by which we will scale the input DSMs. This notebook is used to calculate this scale factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchgeo imports\n",
    "from torchgeo.datasets import RasterDataset\n",
    "from torchgeo.samplers import RandomBatchGeoSampler, Units\n",
    "\n",
    "# GIS imports\n",
    "import rasterio\n",
    "\n",
    "# misc imports\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import torch  # for reproducibility\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user specifies path of processed rasters generated in the previous notebook\n",
    "output_data_path = Path('/mnt/working/karthikv/DeepDEM/data/mt_baker/WV01_20150911_1020010042D39D00_1020010043455300/processed_rasters')\n",
    "dsm_file = output_data_path / 'final_asp_dsm.tif'\n",
    "\n",
    "assert dsm_file.exists(), \"DSM file not found!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scale factor we want to calculate is the standard deviation seen in heights calculated for our training chips, filtered for outliers (values within the 5th-95th percentile). To do this, we randomly sample chips across the training area of our input DSM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHIP_SIZES=[256, 512]\n",
    "\n",
    "for CHIP_SIZE in CHIP_SIZES:\n",
    "    mtbaker_asp_dem = RasterDataset(str(dsm_file))\n",
    "    sampler = RandomBatchGeoSampler(mtbaker_asp_dem, size=CHIP_SIZE, units=Units.PIXELS, batch_size=32, length=5000)\n",
    "\n",
    "    def return_sample_std(batch):\n",
    "        std_values = []\n",
    "        for b in batch:\n",
    "            minx, maxx, miny, maxy, _, _ = b\n",
    "            with rasterio.open(dsm_file) as ds:\n",
    "                img = ds.read(1, window=rasterio.windows.from_bounds(minx, miny, maxx, maxy, transform=ds.transform)).flatten()\n",
    "                img = np.ma.masked_where(img == ds.nodata, img)\n",
    "                std_values.append(np.std(img))\n",
    "\n",
    "        return std_values\n",
    "\n",
    "    std_values = sum(map(return_sample_std, sampler), [])\n",
    "            \n",
    "    lower_percentile, upper_percentile = np.percentile(std_values,  5), np.percentile(std_values,  95)\n",
    "    std_values = np.ma.masked_where((std_values < lower_percentile) & (std_values > upper_percentile), std_values)\n",
    "\n",
    "    gsf = np.nanmean(std_values)\n",
    "    print(f\"Global scale factor for dataset DSM@patch size = ({CHIP_SIZE}x{CHIP_SIZE}): \", gsf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the WV01 Mt Baker dataset (20150911), the DSM scale factor is about 34.81 for a chip size of (256x256) pixels, and 63.67 at (512x512)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_dem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
