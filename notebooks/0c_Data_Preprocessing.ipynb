{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook steps through data preprocessing steps needed prior to DSM refinement. In particular:\n",
    "\n",
    "- Crop all raster extents to the same bounds\n",
    "- Reproject and resample all rasters to the same CRS and resolution\n",
    "- Create a no-data mask file\n",
    "- Calculate NDVI from HLS data and write out a new raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GIS imports\n",
    "import rasterio\n",
    "from rasterio.windows import from_bounds\n",
    "from rasterio.warp import Resampling, reproject, transform_bounds\n",
    "from shapely.geometry import box\n",
    "\n",
    "# misc imports\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from skimage import exposure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our input dataset comprises of two ortho images, the initial DSM generated from these images (in our case, using [ASP](https://stereopipeline.readthedocs.io/en/latest/introduction.html)), co-incident NDVI values (derived from [HLS data](https://hls.gsfc.nasa.gov/)), and finally triangulation errors and nodata values obtained from the ASP processing of the ortho images.\n",
    "\n",
    "Users will need to specify the path to the rasters as well as the specific filenames below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user specified filepaths and filenames\n",
    "input_data_path = Path('/mnt/working/karthikv/DeepDEM/data/mt_baker/WV01_20150911_1020010042D39D00_1020010043455300/original_rasters') # specify folder path containing source rasters\n",
    "common_data_path = Path('/mnt/working/karthikv/DeepDEM/data/common_files')\n",
    "output_data_path = input_data_path.parent / 'processed_rasters' # preprocessed data will be written to this location\n",
    "output_data_path.mkdir(exist_ok=True)\n",
    "\n",
    "# specify filenames for the ortho images, initial DSM, HLS data, triangulation errors, and the ground truth LIDAR data\n",
    "ortho_channel1_file = '1020010042D39D00_left_ortho_1.0m_final_aligned_to_REFDEM.tif'\n",
    "ortho_channel2_file = '1020010043455300_right_ortho_1.0m_final_aligned_to_REFDEM.tif'\n",
    "initial_dsm_file = '20150911_2050_1020010042D39D00_1020010043455300_1.0m-DEM_trans_dx+2.40m_dy+1.51m_dz-0.36m__ASP_wt_avg_extrapolation_search_rad25_num_pass_3.tif'\n",
    "hls_red_band_file = common_data_path / 'HLS.L30.T10UEV.2015254T185445.v2.0.B04.tif'\n",
    "hls_nir_band_file = common_data_path / 'HLS.L30.T10UEV.2015254T185445.v2.0.B05.tif'\n",
    "triangulation_error_file = '20150911_2050_1020010042D39D00_1020010043455300_1.0m-IntersectionErr_trans_dx+2.40m_dy+1.51m_dz+0.00m_final_extent.tif'\n",
    "lidar_data_file = '/mnt/working/karthikv/DeepDEM/data/common_files/merged_dsm.tif' \n",
    "\n",
    "\n",
    "# We define a dictionary using the user defined values provided above\n",
    "input_file_dict = {\n",
    "    'ortho_channel1':ortho_channel1_file,\n",
    "    'ortho_channel2':ortho_channel2_file,\n",
    "    'asp_dsm':initial_dsm_file,\n",
    "    'hls_red':hls_red_band_file,\n",
    "    'hls_nir':hls_nir_band_file,\n",
    "    'triangulation_error':triangulation_error_file,\n",
    "    'lidar_dtm_data':lidar_data_file\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_folder_path = input_data_path / 'tmp'\n",
    "tmp_folder_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open files, create shapely polygons from bounds, intersect them and find common overlap\n",
    "# Then perform a windowed read of the files, and write out the cropped rasters\n",
    "\n",
    "with rasterio.open(input_data_path / ortho_channel1_file) as ds:\n",
    "    reference_crs = ds.crs\n",
    "    reference_x_resolution = abs(ds.transform[0])\n",
    "    reference_y_resolution = abs(ds.transform[4])\n",
    "\n",
    "shapes = []\n",
    "for key, value in input_file_dict.items():\n",
    "    with rasterio.open(input_data_path / value) as ds:\n",
    "        src_crs = ds.crs\n",
    "        shapes.append(box(*transform_bounds(src_crs, reference_crs, *ds.bounds)))\n",
    "\n",
    "intersection_shape =  shapes[0]\n",
    "for s in shapes:\n",
    "    intersection_shape = intersection_shape.intersection(s)\n",
    "\n",
    "print(f\"Bounds of intersection area across rasters in {reference_crs}: \", intersection_shape.bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now perform windowed reads of the rasters and write out new files\n",
    "\n",
    "def write_output_rasters(key_val):\n",
    "\n",
    "    window_read_bounds = intersection_shape.bounds\n",
    "\n",
    "    key, value = key_val\n",
    "    output_file = output_data_path / f\"final_{key}.tif\"\n",
    "\n",
    "    with rasterio.open(input_data_path / value) as ds:\n",
    "        tmp_profile = ds.profile\n",
    "\n",
    "        if ds.crs != reference_crs:\n",
    "            # if the input data CRS does not match the reference CRS, we perform a windowed read of the \n",
    "            # relevant area, and then reproject + resample the data to the reference CRS.\n",
    "\n",
    "            tmp_window_bounds = transform_bounds(reference_crs, ds.crs, *window_read_bounds)\n",
    "            img = ds.read(1, window=from_bounds(*tmp_window_bounds, transform=ds.transform))\n",
    "            \n",
    "            # this is the transform associated with the windowed read in the reference crs\n",
    "            src_transform = rasterio.transform.from_bounds(*tmp_window_bounds, img.shape[-1], img.shape[-2])\n",
    "            \n",
    "            # this is the transform associated with the windowed read in the reference crs\n",
    "            width = np.round((window_read_bounds[2] - window_read_bounds[0])/reference_x_resolution).astype(int)\n",
    "            height = np.round((window_read_bounds[3] - window_read_bounds[1])/reference_y_resolution).astype(int)\n",
    "\n",
    "            dst_transform = rasterio.transform.from_bounds(*window_read_bounds, width, height)\n",
    "\n",
    "            # create the reprojected+resampled raster\n",
    "            dst_raster = np.zeros((height, width))\n",
    "            \n",
    "            reproject(\n",
    "                source=img, \n",
    "                destination=dst_raster, \n",
    "                src_transform=src_transform, \n",
    "                src_crs=ds.crs, \n",
    "                dst_transform=dst_transform, \n",
    "                dst_crs=reference_crs,\n",
    "                resampling=Resampling.nearest\n",
    "            )\n",
    "\n",
    "            # we will write out this img temporarily to perform a windowed read\n",
    "            tmp_profile_copy = tmp_profile.copy()\n",
    "            tmp_profile_copy.update({\n",
    "                'transform':dst_transform,\n",
    "                'width':width,\n",
    "                'height':height,\n",
    "                'crs':reference_crs\n",
    "            })\n",
    "\n",
    "            with rasterio.open(tmp_folder_path/'tmp.tif', 'w', **tmp_profile_copy) as output:\n",
    "                output.write(dst_raster.reshape(1, *dst_raster.shape))\n",
    "\n",
    "            # perform windowed read\n",
    "            with rasterio.open(tmp_folder_path/'tmp.tif') as input:\n",
    "                dst_raster = input.read(1, window=from_bounds(*window_read_bounds, transform=input.transform))\n",
    "\n",
    "            # delete temporary files\n",
    "            (tmp_folder_path/'tmp.tif').unlink()\n",
    "\n",
    "        else:\n",
    "            dst_raster = ds.read(1, window=from_bounds(*window_read_bounds, transform=ds.transform))\n",
    "\n",
    "    transform = rasterio.transform.from_bounds(*window_read_bounds, width=dst_raster.shape[-1], height=dst_raster.shape[-2])\n",
    "\n",
    "    tmp_profile.update({\n",
    "        'width':dst_raster.shape[-1],\n",
    "        'height':dst_raster.shape[-2],\n",
    "        'transform':transform,\n",
    "        'crs':reference_crs,\n",
    "        'blockxsize':256, \n",
    "        'blockysize':256,\n",
    "        'compress': 'deflate',\n",
    "        'tiled': True,\n",
    "        })\n",
    "    \n",
    "    nodata_mask = np.where(dst_raster == tmp_profile['nodata'], 1, 0)\n",
    "    \n",
    "    # change nodata value from -9999 for triangulation error\n",
    "    if key == 'triangulation_error':\n",
    "        dst_raster = np.where(dst_raster==tmp_profile['nodata'], -1, dst_raster)\n",
    "        tmp_profile.update({\n",
    "            'nodata':-1\n",
    "        })\n",
    "    \n",
    "    with rasterio.open(output_file, 'w', **tmp_profile) as ds:\n",
    "        ds.write(dst_raster.reshape(1, *dst_raster.shape))\n",
    "    \n",
    "    print(f\"Written out: {output_file.name}\")\n",
    "\n",
    "    return nodata_mask\n",
    "\n",
    "nodata_mask_list = list(map(write_output_rasters, tqdm(list(input_file_dict.items()))))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate no data values\n",
    "\n",
    "nodata_mask = np.ones((1, nodata_mask_list[0].shape[0], nodata_mask_list[0].shape[1]), dtype=np.uint8)\n",
    "for x in nodata_mask_list:\n",
    "    nodata_mask = np.where(x==1, 0, nodata_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate NDVI values from the HLS data and write them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nir_file = output_data_path / 'final_hls_nir.tif'\n",
    "red_file = output_data_path / 'final_hls_red.tif'\n",
    "\n",
    "assert nir_file.exists(), \"Error, processed HLS NIR band file missing\"\n",
    "assert red_file.exists(), \"Error, processed HLS red band file missing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(nir_file) as ds:\n",
    "    profile = ds.profile\n",
    "    nodata_value = ds.profile['nodata']\n",
    "    nir_img = ds.read(1)\n",
    "    nir_img = np.ma.masked_where(nir_img == nodata_value, nir_img)\n",
    "\n",
    "with rasterio.open(red_file) as ds:\n",
    "    nodata_value = ds.profile['nodata']\n",
    "    red_img = ds.read(1)\n",
    "    red_img = np.ma.masked_where(red_img == nodata_value, red_img)\n",
    "\n",
    "ndvi = (nir_img - red_img)/(nir_img + red_img)\n",
    "\n",
    "# append to nodata mask\n",
    "nodata_mask = np.where(((ndvi<-1) | (ndvi>1)), 0, nodata_mask)\n",
    "\n",
    "# set bad NDVI values to zero. This won't change training/inference since\n",
    "# nodata mask will govern loss calculations\n",
    "ndvi = np.where(((ndvi<-1) | (ndvi>1)), 0, ndvi)\n",
    "\n",
    "# ndvi can take float values\n",
    "profile.update({'dtype':ndvi.dtype})\n",
    "\n",
    "with rasterio.open(output_data_path/\"final_ndvi.tif\", 'w', **profile) as ds:\n",
    "    ds.write(ndvi.reshape(1, *ndvi.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write out the calculated no-data mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(output_data_path / 'final_ortho_channel1.tif') as ds:\n",
    "    profile = ds.profile\n",
    "\n",
    "profile.update({'dtype':str(nodata_mask.dtype), 'nodata':None})\n",
    "\n",
    "with rasterio.open(output_data_path/'final_nodata_mask.tif', 'w', **profile) as dst:\n",
    "    dst.write(nodata_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply adaptive histogram equalization to the ortho images. This improves image contrast, and scales intensity to values between [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = ['final_ortho_channel1.tif', 'final_ortho_channel2.tif']\n",
    "\n",
    "for f in filename:\n",
    "    with rasterio.open(output_data_path / f) as ds:\n",
    "        img = ds.read()\n",
    "        nodata_mask = np.where(img==ds.profile['nodata'], 0, 1)\n",
    "        img = exposure.equalize_adapthist(img).astype(np.float32)*nodata_mask # multiply mask to ensure no-data pixels are zeroed out\n",
    "        profile = ds.profile\n",
    "\n",
    "    profile.update({\n",
    "        'dtype':img.dtype\n",
    "    })\n",
    "\n",
    "    with rasterio.open(output_data_path / f, 'w', **profile) as ds:\n",
    "        ds.write(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete processed HLS NIR and Red images since NDVI file has been written out\n",
    "\n",
    "for key in ['hls_red', 'hls_nir']:\n",
    "    output_file = output_data_path / f\"final_{key}.tif\"\n",
    "    output_file.unlink()\n",
    "    output_file = output_data_path / f\"final_{key}.tif.aux.xml\"\n",
    "    output_file.unlink()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_dem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
